<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Straw in a Haystack</title>
    <link rel="alternate" type="application/rss+xml" title="RSS"
      href="http://jaihirsch.github.io/straw-in-a-haystack/">
    <base href="/straw-in-a-haystack">
    <link rel="alternate" type="application/rss+xml" href="rss-feed">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Random thoughts on programming, MongoDB, and more by Jai Hirsch">

    <link href="/straw-in-a-haystack/css/bootstrap.css" rel="stylesheet">
    <link href="/straw-in-a-haystack/css/pygments/default.css" rel="stylesheet">
    <style>
      body {
        padding-top: 60px;
      }
    </style>
    <link href="/straw-in-a-haystack/css/bootstrap-theme.css" rel="stylesheet">
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-47701133-1', 'jaihirsch.github.io');
      ga('send', 'pageview');

   </script>
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>

  <body>    
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-header">
        <a class="brand" href="http://jaihirsch.github.io/straw-in-a-haystack/">Straw in a Haystack</a>
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
  
      <div class="navbar-collapse collapse">
        <ul class="nav navbar-nav">
          <li class="active"><a href="/straw-in-a-haystack/">Home</a></li>
          <li><a href="/straw-in-a-haystack/about">About</a></li>
        </ul>
     </div>
  </nav>

    <div class="container">

<div class="container">
  <div class="col-xs-8">
    <!-- This loops through the paginated posts -->

<h1><a href="/straw-in-a-haystack/mongodb/2016/05/09/replicating-mongodb-to-elasticsearch-using-flink/ ">Replicating the MongoDB oplog to Elasticsearch using Apache Flink</a></h1>
<p class="author">
  <span class="date">2016-05-09 00:00:00 -0500</span>
</p>
<div class="content">
  <h2>Replicating the MongoDB oplog to Elasticsearch using Apache Flink</h2>

<p>I have been tinkering around with RxJava and reactive streams lately, and I decided to revisit one of my older
projects for tailing the MongoDB oplog and used RabbitMQ to broadcast out changes to a sharded MongoDB Cluster.  In this
    installment of tailing the oplog I decided not to use RabbitMQ, instead I decided to try out Apache Flink.  Flink
    appears to play in the same domain as Spark Streaming.  I am not endorsing one over the other in this blog, and
    it is always important to be constantly evaluating different technologies to ensure you make the right decisions.
    I am using Apache Flink to wire the streaming data source (MongoDB oplog tail) and an Elasticsearch sink.
    </p>
    <p>All source code for this project may be found
        <a href="https://github.com/JaiHirsch/flink-mingo-tail" target="new">here</a>.  Also, before we get started:  All code used here
        is not production quality.  It was not test driven and was written as a thought experiment on a rainy
        Friday afternoon as a way to learn more about
        <a href="http://www.reactive-streams.org" target="new">reactive streams</a>,
        <a hreaf="https://github.com/ReactiveX/RxJava" target="new">RxJava</a>,
        <a href ="https://flink.apache.org" target="new">Apache Flink</a>, and
        <a href="https://www.elastic.co" target="new"> Elasticsearh.</a>

</p>

<p>I will not go deep into the dirty details of oplog tailing other than to say it is much easier with the reactive
    driver. Lets do, however, take a quick look at getting an event publisher using the reactive driver and some of
    syntax differences using the new generation of the MongoDB driver.</p>
<p>This snippet of code is inside a loop that is getting a publisher for every replica set inside of a sharded
    cluster.</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">FindPublisher</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;</span> <span class="n">oplogPublisher</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="na">getClient</span><span class="o">().</span><span class="na">getDatabase</span><span class="o">(</span><span class="s">&quot;local&quot;</span><span class="o">)</span>
    <span class="o">.</span><span class="na">getCollection</span><span class="o">(</span><span class="s">&quot;oplog.rs&quot;</span><span class="o">).</span><span class="na">find</span><span class="o">().</span><span class="na">filter</span><span class="o">(</span><span class="n">getQueryFilter</span><span class="o">(</span><span class="n">tsCollection</span><span class="o">,</span> <span class="n">client</span><span class="o">))</span>
    <span class="o">.</span><span class="na">sort</span><span class="o">(</span><span class="k">new</span> <span class="nf">Document</span><span class="o">(</span><span class="s">&quot;$natural&quot;</span><span class="o">,</span> <span class="mi">1</span><span class="o">)).</span><span class="na">cursorType</span><span class="o">(</span><span class="n">CursorType</span><span class="o">.</span><span class="na">TailableAwait</span><span class="o">);</span></code></pre></figure>


<p>It is also important to understand the filters that are generated though the getQueryFilter() call:
<ul>
    <li>In this example I am using the same MongoDB cluster to track the last operation timestamps. So filtering out
        the collection these are being written to is VERY important, otherwise this creates a pretty bad ass infinite
        loop. On a side note, it is probably not a good idea to have your ops time collection on the same server you
        are tailing as it takes up a lot of space in the oplog.
    </li>
    <li>
        The next filter filters out “no-ops” that show up in the oplog, this will keep down the clutter as we are
        only looking for CRUD ops.
    </li>
    <li>
        Next we filter out fromMigrate, this removes operations from chunk migration and prevents false positives
        while tailing.

    </li>
    <li>
        Finally we get the last operation timestamp, this prevents the entire oplog from replaying every time we connect.
    </li>
</ul>
</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">private</span> <span class="n">Bson</span> <span class="nf">getQueryFilter</span><span class="o">(</span><span class="n">MongoCollection</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;</span> <span class="n">tsCollection</span><span class="o">,</span> <span class="n">MongoClientWrapper</span> <span class="n">client</span><span class="o">)</span> <span class="o">{</span>
<span class="k">return</span> <span class="nf">and</span><span class="o">(</span><span class="n">ne</span><span class="o">(</span><span class="s">&quot;ns&quot;</span><span class="o">,</span> <span class="s">&quot;time_d.repl_time&quot;</span><span class="o">),</span> <span class="n">ne</span><span class="o">(</span><span class="s">&quot;op&quot;</span><span class="o">,</span> <span class="s">&quot;n&quot;</span><span class="o">),</span> <span class="n">exists</span><span class="o">(</span><span class="s">&quot;fromMigrate&quot;</span><span class="o">,</span> <span class="kc">false</span><span class="o">),</span><span class="n">getFilterLastTimeStamp</span><span class="o">(</span><span class="n">tsCollection</span><span class="o">,</span> <span class="n">client</span><span class="o">));</span>
<span class="o">}</span></code></pre></figure>

<p>The time stamp filter keeps us from replaying the entire oplog every time we connect, if you notice this is
    querying MongoDB by the host, this corresponds to each of the host's server info that comes from a the sharded
    cluster.</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">private</span> <span class="n">Bson</span> <span class="nf">getFilterLastTimeStamp</span><span class="o">(</span><span class="n">MongoCollection</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;</span> <span class="n">tsCollection</span><span class="o">,</span> <span class="n">MongoClientWrapper</span> <span class="n">client</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">Document</span> <span class="n">lastTimeStamp</span> <span class="o">=</span> <span class="n">tsCollection</span><span class="o">.</span><span class="na">find</span><span class="o">(</span><span class="k">new</span> <span class="nf">Document</span><span class="o">(</span><span class="s">&quot;host&quot;</span><span class="o">,</span> <span class="n">client</span><span class="o">.</span><span class="na">getHost</span><span class="o">())).</span><span class="na">limit</span><span class="o">(</span><span class="mi">1</span><span class="o">).</span><span class="na">first</span><span class="o">();</span>
    <span class="k">return</span> <span class="nf">getTimeQuery</span><span class="o">(</span><span class="n">lastTimeStamp</span> <span class="o">==</span> <span class="kc">null</span> <span class="o">?</span> <span class="kc">null</span> <span class="o">:</span> <span class="o">(</span><span class="n">BsonTimestamp</span><span class="o">)</span> <span class="n">lastTimeStamp</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">&quot;ts&quot;</span><span class="o">));</span>
<span class="o">}</span>

<span class="kd">private</span> <span class="n">Bson</span> <span class="nf">getTimeQuery</span><span class="o">(</span><span class="n">BsonTimestamp</span> <span class="n">lastTimeStamp</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">lastTimeStamp</span> <span class="o">==</span> <span class="kc">null</span> <span class="o">?</span> <span class="k">new</span> <span class="nf">Document</span><span class="o">()</span> <span class="o">:</span> <span class="n">gt</span><span class="o">(</span><span class="s">&quot;ts&quot;</span><span class="o">,</span> <span class="n">lastTimeStamp</span><span class="o">);</span>
<span class="o">}</span></code></pre></figure>

<p>
The MongoDB reactive driver uses the reactive streams standard. Reactive streams is rumored to be implemented in the Java 9
    release. This posed a bit of an issue while I was trying to bind it the RxJava, but there is a great translation
    library that wires the two approaches together,
    <a href="https://github.com/ReactiveX/RxJavaReactiveStreams" target="new">RxJavaReactiveStreams</a>.
    The bindPublisherToObservable method is called in a loop and attaches a subscriber onto an oplog tail per mongod
    that has been passed in.  This will create a thread per mongod. The code then uses a ConcurrentHashMap to
    increment the number of times a given operation is observed.  Once the map count is equal the replica set depth
    (the number of replicas per shard) the given operation has fully replicated to all replica sets.  At this point
    the operation is placed on an ArrayBlockingQueue opsQueue and removed from the map.  The opsQueue is used by the
    Flink data source to collect the operation and place it on the Flink work flow.
</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">String</span> <span class="n">OPLOG_TIMESTAMP</span> <span class="o">=</span> <span class="s">&quot;ts&quot;</span><span class="o">;</span>
<span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">String</span> <span class="n">OPLOG_ID</span> <span class="o">=</span> <span class="s">&quot;h&quot;</span><span class="o">;</span>

<span class="o">...</span>

<span class="kd">private</span> <span class="kt">void</span> <span class="nf">bindPublisherToObservable</span><span class="o">(</span><span class="n">Entry</span><span class="o">&lt;/</span><span class="n">String</span><span class="o">,</span> <span class="n">FindPublisher</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;&gt;</span> <span class="n">oplogPublisher</span><span class="o">,</span>
    <span class="n">ExecutorService</span> <span class="n">executor</span><span class="o">,</span> <span class="n">MongoCollection</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;</span> <span class="n">tsCollection</span><span class="o">)</span> <span class="o">{</span>
    <span class="n">RxReactiveStreams</span><span class="o">.</span><span class="na">toObservable</span><span class="o">(</span><span class="n">oplogPublisher</span><span class="o">.</span><span class="na">getValue</span><span class="o">())</span>
      <span class="o">.</span><span class="na">subscribeOn</span><span class="o">(</span><span class="n">Schedulers</span><span class="o">.</span><span class="na">from</span><span class="o">(</span><span class="n">executor</span><span class="o">)).</span><span class="na">subscribe</span><span class="o">(</span><span class="n">t</span> <span class="o">-&gt;</span> <span class="o">{</span>
        <span class="k">try</span> <span class="o">{</span>
            <span class="n">putOperationOnOpsQueue</span><span class="o">(</span><span class="n">oplogPublisher</span><span class="o">,</span> <span class="n">tsCollection</span><span class="o">,</span> <span class="n">t</span><span class="o">);</span>
        <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">InterruptedException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{}</span>
    <span class="o">});</span>
<span class="o">}</span>

<span class="kd">private</span> <span class="kt">void</span> <span class="nf">putOperationOnOpsQueue</span><span class="o">(</span><span class="n">Entry</span><span class="o">&lt;/</span><span class="n">String</span><span class="o">,</span> <span class="n">FindPublisher</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;&gt;</span> <span class="n">publisher</span><span class="o">,</span>
    <span class="n">MongoCollection</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;</span> <span class="n">tsCollection</span><span class="o">,</span> <span class="n">Document</span> <span class="n">t</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">InterruptedException</span> <span class="o">{</span>
    <span class="n">updateHostOperationTimeStamp</span><span class="o">(</span><span class="n">tsCollection</span><span class="o">,</span> <span class="n">t</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">OPLOG_TIMESTAMP</span><span class="o">,</span> <span class="n">BsonTimestamp</span><span class="o">.</span><span class="na">class</span><span class="o">),</span> <span class="n">publisher</span><span class="o">.</span><span class="na">getKey</span><span class="o">());</span>
    <span class="n">putOperationOnOpsQueueIfFullyReplicated</span><span class="o">(</span><span class="n">t</span><span class="o">);</span>
<span class="o">}</span>

<span class="kd">private</span> <span class="kt">void</span> <span class="nf">putOperationOnOpsQueueIfFullyReplicated</span><span class="o">(</span><span class="n">Document</span> <span class="n">t</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">InterruptedException</span> <span class="o">{</span>
    <span class="n">Long</span> <span class="n">opKey</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="na">getLong</span><span class="o">(</span><span class="n">OPLOG_ID</span><span class="o">);</span>
    <span class="n">documentCounter</span><span class="o">.</span><span class="na">putIfAbsent</span><span class="o">(</span><span class="n">opKey</span><span class="o">,</span> <span class="k">new</span> <span class="nf">AtomicInteger</span><span class="o">(</span><span class="mi">1</span><span class="o">));</span>
    <span class="k">if</span> <span class="o">(</span><span class="n">documentCounter</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">opKey</span><span class="o">).</span><span class="na">getAndIncrement</span><span class="o">()</span> <span class="o">&gt;=</span> <span class="n">replicaDepth</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">opsQueue</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">t</span><span class="o">);</span>
        <span class="n">documentCounter</span><span class="o">.</span><span class="na">remove</span><span class="o">(</span><span class="n">opKey</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span></code></pre></figure>

<p>
    The code to run the oplog tail to elastic search replication is fairly simple:
</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">Exception</span> <span class="o">{</span>
    <span class="n">StreamExecutionEnvironment</span> <span class="n">see</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>
    <span class="n">DataStream</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;</span> <span class="n">ds</span> <span class="o">=</span> <span class="n">see</span><span class="o">.</span><span class="na">addSource</span><span class="o">(</span><span class="k">new</span> <span class="nf">MongoDBOplogSource</span><span class="o">(</span><span class="s">&quot;host&quot;</span><span class="o">,</span> <span class="n">port</span><span class="o">));</span>
    <span class="n">ds</span><span class="o">.</span><span class="na">addSink</span><span class="o">(</span><span class="k">new</span> <span class="n">PrintSinkFunction</span><span class="o">&lt;/</span><span class="n">Document</span><span class="o">&gt;());</span>
    <span class="n">ds</span><span class="o">.</span><span class="na">addSink</span><span class="o">(</span><span class="k">new</span> <span class="nf">ElasticsearchEmbeddedNodeSink</span><span class="o">(</span><span class="s">&quot;cluster.name&quot;</span><span class="o">).</span><span class="na">getElasticSink</span><span class="o">());</span>
    <span class="n">see</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="s">&quot;MongoDB Sharded Oplog Tail&quot;</span><span class="o">);</span>
<span class="o">}</span></code></pre></figure>

<p> First we create the stream execution environment, next we add the MongoDB oplog source, and finally print and
    Elasticsearch sinks are added.  The PrintSinkFunction function was used to show how easy it is to attach multiple
    sinks to the execution environment.
</p>

<p>
    For this example I have not fully flushed out the Elasticsearch sink.  It is actually indexing the oplog documents
    and not creating a synchronized copy of the MongoDB collections.  I decided not to fully implement the Elasticsearh
    code as there are already a few decent projects out there that do this.  Also, I was only able to the the 1.7x
    Apache Flink connector for Elasticsearch working, as I unable find the 2.x connector on Maven Central.
</p>

<p>
    I had quite a bit of fun taking the reactive driver for MongoDB for a spin around the block.  Stream and event
    driven programming is something we all need to be aware of and it is just as important in the server side and
    big data worlds as it is in for Android and other font end development.
</p>

</div>

<h1><a href="/straw-in-a-haystack/mongodb/2016/02/15/gridfs-duplicate-files/ ">Finding Duplicate Files in GridFS</a></h1>
<p class="author">
  <span class="date">2016-02-15 00:00:00 -0600</span>
</p>
<div class="content">
  <h2>Finding Duplicate Files in GridFS</h2>

<p>I have been working on several large scale GridFS projects lately and the topic of finding, removing, and preventing
    duplicate files keeps coming up. As such, here are some of my findings and code samples around this topic.</p>
<br>
<p>First of all, what is GridFS?</p>
<blockquote cite="https://docs.mongodb.org/manual/reference/glossary/#term-gridfs">
    A convention for storing large files in a MongoDB database. All of the official MongoDB drivers support this
    convention, as does the mongofiles program.
    <p>...</p>
    GridFS stores files in two collections:
    <ul>
        <li>chunks stores the binary chunks. For details, see <a
                href="https://docs.mongodb.org/manual/core/gridfs/#the-chunks-collection">The chunks Collection.</a>
        </li>
        <li>files stores the file’s metadata. For details, see <a
                href="https://docs.mongodb.org/manual/core/gridfs/#the-files-collection">The files Collection.</a></li>
    </ul>
    GridFS places the collections in a common bucket by prefixing each with the bucket name. By default, GridFS uses two
    collections with a bucket named fs:
    <ul>
        <li>fs.files</li>
        <li>fs.chunks</li>
    </ul>
</blockquote>


<p>-- <a href="https://docs.mongodb.org/manual/reference/glossary/#term-gridfs">MongoDB Manual</a></p>

<p>
    Within the the files collection GridFS gives you the md5 hash of the file when it is uploaded.
    This does appear to
    a perfect candidate for identifying duplicate files and when you search google on this topic you will quickly find
    examples, gists and stackoverflow posts on how to use the md5 to identify duplicate files.
</p>

<p>Example file header:</p>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="p">{</span>
   <span class="s2">&quot;_id&quot;</span> <span class="o">:</span> <span class="nx">ObjectId</span><span class="p">(</span><span class="s2">&quot;56bf88d782419304bf212014&quot;</span><span class="p">),</span>
   <span class="s2">&quot;filename&quot;</span> <span class="o">:</span> <span class="s2">&quot;foo.txt&quot;</span><span class="p">,</span>
   <span class="s2">&quot;aliases&quot;</span> <span class="o">:</span> <span class="kc">null</span><span class="p">,</span>
   <span class="s2">&quot;chunkSize&quot;</span> <span class="o">:</span> <span class="nx">NumberLong</span><span class="p">(</span><span class="mi">261120</span><span class="p">),</span>
   <span class="s2">&quot;uploadDate&quot;</span> <span class="o">:</span> <span class="nx">ISODate</span><span class="p">(</span><span class="s2">&quot;2016-02-13T19:49:43.575Z&quot;</span><span class="p">),</span>
   <span class="s2">&quot;length&quot;</span> <span class="o">:</span> <span class="nx">NumberLong</span><span class="p">(</span><span class="mi">541</span><span class="p">),</span>
   <span class="s2">&quot;contentType&quot;</span> <span class="o">:</span> <span class="kc">null</span><span class="p">,</span>
   <span class="s2">&quot;md5&quot;</span> <span class="o">:</span> <span class="s2">&quot;3813e09a727083f74b45fe7f5b253c07&quot;</span>
<span class="p">}</span></code></pre></figure>

<p>
    Here is an example using the aggregation pipeline that groups my md5, pushes the _ids into an array, and matches
    where there are two or more md5 matches:
    
<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript">    <span class="nx">db</span><span class="p">.</span><span class="nx">fs</span><span class="p">.</span><span class="nx">files</span><span class="p">.</span><span class="nx">aggregate</span><span class="p">([{</span><span class="nx">$group</span><span class="o">:</span><span class="p">{</span><span class="nx">_id</span><span class="o">:</span><span class="s1">&#39;$md5&#39;</span><span class="p">,</span><span class="nx">count</span><span class="o">:</span><span class="p">{</span><span class="nx">$sum</span><span class="o">:</span><span class="mi">1</span><span class="p">},</span><span class="nx">ids</span><span class="o">:</span><span class="p">{</span><span class="nx">$push</span><span class="o">:</span><span class="s1">&#39;$_id&#39;</span><span class="p">}}},{</span><span class="nx">$match</span><span class="o">:</span><span class="p">{</span><span class="nx">count</span><span class="o">:</span><span class="p">{</span><span class="nx">$gte</span><span class="o">:</span><span class="mi">2</span><span class="p">}}}])</span>
    </code></pre></figure>

    It is also important to note that for this aggregation to work well there should be an index on md5 field. This
    could
    be a very costly index on a large implementation.
</p>

<p>
    So if there are so many posts out there and if this is a solved problem then why am I writing about it? Because
    you should never trust an md5 hash, especially for binary files. Lets take a moment for a closer look at the
    md5.
</p>

<blockquote cite="">
    The MD5 message-digest algorithm is a widely used cryptographic hash function producing a 128-bit (16-byte) hash
    value, typically expressed in text format as a 32 digit hexadecimal number. MD5 has been utilized in a wide variety
    of cryptographic applications, and is also commonly used to verify data integrity.
</blockquote>

<p>-- <a href="https://en.wikipedia.org/wiki/MD5">Wikipedia</a></p>

<p>There are many scholarly works written by much smarter people than I available on the probability of md5 collision,
    so I will not spend much time going over it. I have, however, encountered the consequences of hash
    collisions before and it is never fun.
</p>

<p>Unfortunately, the files that I have dealt with before, that had collisions, are proprietary and I can not share them
    here, so I took to the googles and found some interesting utilities and articles.
</p>

<p>Here are two that I liked in particular:
<ul>
    <li><a href="https://marc-stevens.nl/p/hashclash/">Project HashClash</a></li>
    <li><a href="http://www.mscs.dal.ca/~selinger/md5collision/">MD5 Collision Demo</a></li>
</ul>
</p>

<p>
    To perform my GridFS experiment I used a few jpg files that were created by Nat McHugh and may be found on his
    blog post:

    <a href="http://natmchugh.blogspot.com/2014/10/how-i-created-two-images-with-same-md5.html">How I created two images
        with the same MD5 hash</a> (He created the third one later)
</p>

<p><img src="http://www.fishtrap.co.uk/barry.jpg.coll"></p>

<p><img src="http://www.fishtrap.co.uk/james.jpg.coll"></p>

<p><img src="http://www.fishtrap.co.uk/black.jpg.coll"></p>

<p><br><br></p>

<p>You just can't go wrong with White, Brown, and Black. These images are obviously different, yet they all have the
    same md5 hash. What is more, they also show the same chunkSize and length information in the GridFS files document.
    Due to the fact that these three fields are identical we can not use the aggregation framework as noted thus far
    to identify that they are, in fact, not duplicates.
</p>

<p>There are many ways other ways to identify if two files are duplicates. Lets take a look at one such approach that
    also takes advantage the file streaming aspect of GridFS.</p>

<p><b>The following example code is written in Java and should be considered demo quality only</b>.</p>

<p>First you must get the input stream from the GridFS file</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="n">GridFSDBFile</span> <span class="n">findFile1</span> <span class="o">=</span> <span class="n">gridFS</span><span class="o">.</span><span class="na">find</span><span class="o">(</span><span class="n">docemnt</span><span class="o">);</span>
<span class="n">InputStream</span> <span class="n">is1</span> <span class="o">=</span> <span class="n">findFile1</span><span class="o">.</span><span class="na">getInputStream</span><span class="o">();</span></code></pre></figure>

<p>Once we get both of the input streams we want to compare we can do so as follows:</p>

<figure class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">public</span> <span class="kt">boolean</span> <span class="nf">isDuplicateStream</span><span class="o">(</span><span class="n">InputStream</span> <span class="n">is1</span><span class="o">,</span> <span class="n">InputStream</span> <span class="n">is2</span><span class="o">)</span> <span class="o">{</span>
   <span class="kt">boolean</span> <span class="n">isSame</span> <span class="o">=</span> <span class="kc">true</span><span class="o">;</span>
   <span class="kt">int</span> <span class="n">val1</span><span class="o">,</span> <span class="n">val2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">;</span>
   <span class="k">do</span> <span class="o">{</span>
      <span class="k">try</span> <span class="o">{</span>
         <span class="n">val1</span> <span class="o">=</span> <span class="n">is1</span><span class="o">.</span><span class="na">read</span><span class="o">();</span>
         <span class="n">val2</span> <span class="o">=</span> <span class="n">is2</span><span class="o">.</span><span class="na">read</span><span class="o">();</span>
         <span class="k">if</span> <span class="o">(</span><span class="n">val1</span> <span class="o">!=</span> <span class="n">val2</span><span class="o">)</span> <span class="o">{</span>
            <span class="n">isSame</span> <span class="o">=</span> <span class="kc">false</span><span class="o">;</span>
            <span class="k">break</span><span class="o">;</span>
         <span class="o">}</span>
      <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
         <span class="n">logger</span><span class="o">.</span><span class="na">warn</span><span class="o">(</span><span class="n">e</span><span class="o">.</span><span class="na">getMessage</span><span class="o">());</span>
         <span class="n">isSame</span> <span class="o">=</span> <span class="kc">false</span><span class="o">;</span>
         <span class="k">break</span><span class="o">;</span>
      <span class="o">}</span>

   <span class="o">}</span> <span class="k">while</span> <span class="o">(</span><span class="n">val1</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&amp;&amp;</span> <span class="n">val2</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="o">);</span>  <span class="c1">// important to check both to make sure both are completely read</span>
   <span class="k">return</span> <span class="n">isSame</span><span class="o">;</span>
<span class="o">}</span></code></pre></figure>

<p>I will be the first to admit, this code is rather ugly, but it works well enough for this.  Using streams has the
advantage that you do not have to materialize the file in order to check if they are duplicates.  The downside is that
it will take linear time to scan both files.</p>

<p> The GridFS documentation does mention ranged queries and the ability to skip to arbitrary locations in the file.
This should allow us to write a threaded file validator, so look forward to part two of the post on optimization.</p>

<p>The full project I used to may be found <a href="https://github.com/JaiHirsch/gridfs-duplicate-cleaner">here</a></p>

</div>


<!-- Pagination links -->
<div class="pagination">
  
  <span class="previous">Previous</span>
  
  <span class="page_number ">Page: 1 of 4</span>
  
  <a href="/archives/page/2" class="next">Next</a>
  
</div>


<div class="pagination">
  
  <span>&laquo; Prev</span>
  

  
  
  <em>1</em>
  
  
  
  <a href="/straw-in-a-haystack/archives/page/2">2</a>
  
  
  
  <a href="/straw-in-a-haystack/archives/page/3">3</a>
  
  
  
  <a href="/straw-in-a-haystack/archives/page/4">4</a>
  
  

  
  <a href="/straw-in-a-haystack/archives/page/2">Next &raquo;</a>
  
</div>

  </div>
  <div class="col-xs-4">
    <h3> About Me</h3>

<div class="contact">
  <p>
    Jai Hirsch<br />
    Senior Systems Architect<br />
    CARFAX<br>
    jai.hirsch@gmail.com
  </p>
  </div>
  <div class="contact">
    <p>
      <a href="https://github.com/JaiHirsch">github.com/JaiHirsch</a><br />
      <a href="https://twitter.com/JaiHirsch">twitter.com/JaiHirsch</a><br />
   </p>
</div>

<ul>
  
    <li><a href="/straw-in-a-haystack//mongodb/2016/05/09/replicating-mongodb-to-elasticsearch-using-flink/">Replicating the MongoDB oplog to Elasticsearch using Apache Flink</a></li>
  
    <li><a href="/straw-in-a-haystack//mongodb/2016/02/15/gridfs-duplicate-files/">Finding Duplicate Files in GridFS</a></li>
  
    <li><a href="/straw-in-a-haystack//mongodb/2015/12/04/mongodb-document-validation/">Thoughts on MongoDB 3.2 Document Validation</a></li>
  
    <li><a href="/straw-in-a-haystack//mongodb/2014/09/16/generate-bson-files-with-java/">Generating BSON files with java</a></li>
  
    <li><a href="/straw-in-a-haystack//mongodb/2014/08/18/mongo-oplog-tailing/">Tailing the MongoDB oplog and RabbitMQ</a></li>
  
</ul>

  </div>
</div>

  </body>
</html>