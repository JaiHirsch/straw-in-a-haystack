<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Straw in a Haystack</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Random thoughts on programming, MongoDB, and more">

    <link href="/straw-in-a-haystack/css/bootstrap.css" rel="stylesheet">
    <style>
      body {
        padding-top: 60px;
      }
    </style>
    <link href="/straw-in-a-haystack/css/bootstrap-theme.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>

  <body>    
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="navbar-header">
        <a class="brand" href="/straw-in-a-haystack#">Straw in a Haystack</a>
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
      </div>
  
      <div class="navbar-collapse collapse">
        <ul class="nav navbar-nav">
          <li class="active"><a href="/straw-in-a-haystack/">Home</a></li>
          <li><a href="/straw-in-a-haystack/about">About</a></li>
        </ul>
     </div>
  </nav>

    <div class="container">
<div class="container">
  <div class="col-xs-8">
    <h2>CARFAX vehicle history database migration to MongoDB</h2>
<p class="meta">06 Jan 2014</p>

<div class="post">
<h3>Why we decided to move over eleven billion records from the legacy system</h3>

<p>The legacy vehicle history database is a distributed key value store using an ISAM 
(Indexed Sequential Access Method) database on <a href="http://en.wikipedia.org/wiki/OpenVMS">OpenVMS</a>.  
If the compression was removed 
from the then eleven billion records in the OpenVMS database it would have been over a 
petabyte is size.  Compressed the database is roughly nine terabytes.  This system has 
served CARFAX very well since 1984 when the company was founded, but we were capping out 
at two hundred vehicle history reports per second per production cluster.  The hardware 
cost and technical difficulties of scaling the legacy system were very high.  Thus we 
decided to start looking at different options.
</p>  

<h3>Why we decided to use MongoDB</h3>

<p>As an Agile and extreme programming development shop, CARFAX uses Friday afternoons 
as professional/personal development time.  Several developers began using this time to 
experimenting with traditional RDBMS and NOSQL solutions for the vehicle history database.  
The complexity of the data structures compounded with the size of the database did not lend 
itself to Oracle or MySql.  We also evaluated Cassandra, CouchDB, Riak, and several other 
noSQL solutions (I really wanted to use Neo4J to create a social network of cars).  In the 
end we decided that MongoDB gave us the best of where we wanted to be in the <a href="http://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a>.
</p>  

<h3>Challenges we faced during the migration</h3>

<p>Cleansing, transforming, and inserting eleven billion complex records into a database is no 
small task. To avoid inconsistencies with the ongoing data feeds it was decided that we needed 
to load the existing records into MongoDB in under fifteen days. The legacy records were of 
multiple fixed width formats that required specific interpretations.  Many of the records were 
also inserted using PL/I data structures that have no direct mapping in Java or C and required 
bitmapping and other fun tricks.  Once we wrote the Java and C JNI algorithms to transform the 
old records to the MongoDB format we began our loads.
</p>

<p>MongoDB does not support OpenVMS and as such we had to use remote MongoS processes to load 
the data. Using direct inserts to MongoDB we opened up the queues.  The process was immediately 
computationally bound.  The poor processers on the OpenVMS boxes were running full tilt; we could 
barely even get a command prompt to check on the system.  We managed to throttle back the queues 
so we could get a stable session, but the projected transformation and load time was at forty five 
days.  We let the loads continue but we went back to the drawing board to consider how to solve the 
computational limitations we had hit.
</p>

<p><img src="/straw-in-a-haystack/images/vms_direct_insert.jpg" alt="Direct load to MongoDB"></p>

<h3>The Solution</h3>

<p>The transformation and insertion logic was already pretty clean and stateless so with a modest 
amount work we were able to implement a multi-threaded and multi-JVM RabbitMQ consumer solution.  
We released the queues.  The initial numbers did not look all that promising, but once the shard 
ranges on MongoDB settled in we began to see very impressive load numbers.  We averaged over eleven 
thousand plus writes a second to MongoDB.  We finished the load in just under ten days.  Through 
judicious use of compression of certain fields that are not useful for normal queries or aggregation 
the MongoDB collection was actually slightly smaller than the original database on OpenVMS.
</p>

<p><img src="/straw-in-a-haystack/images/rabbit_insert.jpg" alt="Direct load to MongoDB"></p>

<h3>Successes of the Migration</h3>

<ul>
  <li>Each replica set in the production cluster is capable of producing over two thousand reports a second versus the two hundred reports of the legacy system.
  <li>They legacy system could only answer one question easily:  What are the records for a given vehicle?  Any others queries required a custom C crawler and could take weeks return the answers.  In MongoDB we are able to product many useful reports from the data much faster.
  <li>Setting up a new cluster in the legacy system was an extensive process.  We can stand up a new replica set in MongoDB in about two days.  This is only possible through the use of automation.  Automate EVERYTHING you can.
  <li>Cross datacenter data publication and synchronization was error prone and complicated under the old system.  In MongoDB we are generally only a few seconds behind in our various data centers.
</ul>

<h3>Other Thoughts:</h3>

<ul>
  <li>If you are using a Linux distribution; <b>check that you are not using the default ulimit settings</b>!  We suffered a production crash due to missing this in our initial automation settings.
  <li>Moving terabytes of data though an internal network is a challenge.  Work very closely with your network team to make sure everyone is ready for the traffic.
  <li>Naming conventions for the fields in your MongoDB schema are very important!  Due to the BSON format, remember that every character counts in large implementations.
  <li>Consider your shard keys very carefully!  Indexes on extensive collections use a lot of RAM.  We chose a shard key that maximized read and write performance but it was not the _id field.  This forced us into a two index situation.  The _id index is only used for updates and deletes which are relatively infrequent in this system.
  <li>If you are computationally bound, figure out how to grid out the problem.  RabbitMQ worked well in our case, but carefully consider your problem domain and figure out what is the best solution.  We also considered 0MQ and GridGain either would have been great choices but we decided upon RabbitMQ mostly due to in-house knowledge.  We are currently evaluating moving our distributed load process to Scala and Akka.
  <li>For high volume inserts the more shards the better (and LOTS of RAM)!  Managing those shards can be tricky but it pays off.
  <li>We attempted at one point to break the dataset down into a massive number of collections due to the collection level locking in MongoDB, however, the rebalancing of so many collections destroyed our ability to insert and we reverted back to a single large collection.
  <li>In a large sharded collection make sure that queries that need to return quickly use the shard key.
</ul>

</div>

  </div>
  <div class="col-xs-4">
    <h3> About Me</h3>

<div class="contact">
  <p>
    Jai Hirsch<br />
    Senior Systems Architect<br />
    CARFAX<br>
    jai.hirsch@gmail.com
  </p>
  </div>
  <div class="contact">
    <p>
      <a href="https://github.com/JaiHirsch">github.com/JaiHirsch</a><br />
      <a href="https://twitter.com/JaiHirsch">twitter.com/JaiHirsch</a><br />
   </p>
</div>

<ul>
  
    <li><a href="/straw-in-a-haystack//leadership/2014/01/06/thoughts-on-leadership/">Thoughts On Leadership</a></li>
  
    <li><a href="/straw-in-a-haystack//mongodb/2014/01/06/mongodb-legacy-migration/">CARFAX vehicle history database migration to MongoDB</a></li>
  
</ul>
  </div>
</div>
  </div>

    <script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'strawinahaystack'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
    </script>
    
  </body>
</html>